# Financial Data ETL Pipeline

A production-ready ETL (Extract, Transform, Load) pipeline for collecting, processing, and storing financial market data from multiple sources including stock markets, cryptocurrencies, and economic indicators.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Sources                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Alpha   â”‚  â”‚Finnhub  â”‚  â”‚  FRED   â”‚  â”‚ Crypto  â”‚       â”‚
â”‚  â”‚Vantage  â”‚  â”‚   API   â”‚  â”‚   API   â”‚  â”‚   API   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚           â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Extract Layer          â”‚
        â”‚  - API Clients           â”‚
        â”‚  - Rate Limiting         â”‚
        â”‚  - Error Handling        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Transform Layer        â”‚
        â”‚  - Data Cleaning         â”‚
        â”‚  - Validation            â”‚
        â”‚  - Feature Engineering   â”‚
        â”‚  - Standardization       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Load Layer             â”‚
        â”‚  - Supabase Loader       â”‚
        â”‚  - Batch Processing      â”‚
        â”‚  - Upsert Logic          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Supabase Database      â”‚
        â”‚  - Stock Prices          â”‚
        â”‚  - Crypto Prices         â”‚
        â”‚  - Economic Indicators   â”‚
        â”‚  - ETL Metadata          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- **Multi-Source Data Collection**: Extracts data from:
  - Alpha Vantage (Stock prices, company fundamentals)
  - Finnhub (Real-time market data)
  - FRED (Economic indicators)
  - Cryptocurrency APIs (Bitcoin, Ethereum, etc.)
  - Weather data (for correlational analysis)

- **Robust Data Processing**:
  - Automatic data cleaning and validation
  - Duplicate detection and removal
  - Type conversion and standardization
  - Missing value handling
  - Data quality checks

- **Production-Ready Features**:
  - Airflow orchestration for scheduling
  - Rate limiting and retry logic
  - Comprehensive logging
  - Error handling and alerting
  - Monitoring with Prometheus & Grafana
  - Docker containerization

- **Data Storage**:
  - Supabase (PostgreSQL) for persistent storage
  - Batch loading with upsert capability
  - Metadata tracking for pipeline runs

## ğŸ“‹ Prerequisites

- Python 3.9+
- Docker & Docker Compose
- API Keys for:
  - Alpha Vantage
  - Finnhub
  - FRED
- Supabase account

## ğŸ› ï¸ Installation

### 1. Clone the repository

```bash
git clone <repository-url>
cd financial-etl-pipeline
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Copy the example environment file and fill in your credentials:

```bash
cp .env.example .env
```

Edit `.env` with your API keys:

```env
# Alpha Vantage
ALPHA_VANTAGE_API_KEY=your_key_here

# Finnhub
FINNHUB_API_KEY=your_key_here

# FRED
FRED_API_KEY=your_key_here

# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# Airflow
AIRFLOW_FERNET_KEY=generate_with_python_cryptography
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=admin

# Pipeline Configuration
STOCK_SYMBOLS=AAPL,GOOGL,MSFT,AMZN,TSLA
CRYPTO_SYMBOLS=BTC,ETH,ADA,SOL
FRED_SERIES=GDP,UNRATE,CPIAUCSL
```

### 5. Generate Airflow Fernet Key

```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

## ğŸ³ Docker Deployment

### Start all services

```bash
cd docker
docker-compose up -d
```


This will start:
- PostgreSQL (Airflow metadata)
- Redis (Celery broker)
- Airflow Webserver (port 8080)
- Airflow Scheduler
- Airflow Worker
- Flower (Celery monitoring, port 5555)
- Prometheus (port 9090)
- Grafana (port 3000)

### Access services

- Airflow UI: http://localhost:8080
- Flower: http://localhost:5555
- Grafana: http://localhost:3000
- Prometheus: http://localhost:9090

### Stop services

```bash
docker-compose down
```

### View logs

```bash
docker-compose logs -f airflow-scheduler
```

## ğŸ’» Usage

### Running the ETL Pipeline

#### Using Airflow (Recommended for Production)

1. Access Airflow UI at http://localhost:8080
2. Enable the `financial_data_etl_pipeline` DAG
3. Trigger manually or wait for scheduled run (6 PM weekdays)

#### Running Standalone Scripts

Extract stock data:
```bash
python -m src.extract.alpha_vantage
```

Run full pipeline:
```bash
python scripts/run_etl.py
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_extract.py
```

## ğŸ“ Project Structure

```
financial-etl-pipeline/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ database.yaml           # Database configuration
â”‚   â”œâ”€â”€ pipeline_config.yaml    # Pipeline settings
â”‚   â”œâ”€â”€ settings.py             # Application settings
â”‚   â””â”€â”€ sources.yaml            # Data source configs
â”œâ”€â”€ docker/                      # Docker configuration
â”‚   â”œâ”€â”€ docker-compose.yml      # Service orchestration
â”‚   â””â”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ extract/                # Data extraction
â”‚   â”‚   â”œâ”€â”€ base_extractor.py  # Base class for extractors
â”‚   â”‚   â”œâ”€â”€ alpha_vantage.py   # Stock data extractor
â”‚   â”‚   â”œâ”€â”€ finnhub.py          # Market data extractor
â”‚   â”‚   â”œâ”€â”€ fred.py             # Economic data extractor
â”‚   â”‚   â”œâ”€â”€ crypto.py           # Crypto data extractor
â”‚   â”‚   â””â”€â”€ weather.py          # Weather data extractor
â”‚   â”œâ”€â”€ transform/              # Data transformation
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py    # Data cleaning logic
â”‚   â”‚   â”œâ”€â”€ feature_engineer.py # Feature creation
â”‚   â”‚   â”œâ”€â”€ standardizer.py    # Data standardization
â”‚   â”‚   â””â”€â”€ validator.py        # Data validation
â”‚   â”œâ”€â”€ load/                   # Data loading
â”‚   â”‚   â”œâ”€â”€ data_models.py     # Pydantic models
â”‚   â”‚   â””â”€â”€ supabase_loader.py # Database loader
â”‚   â”œâ”€â”€ orchestration/          # Pipeline orchestration
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â”‚       â””â”€â”€ etl_pipeline.py # Airflow DAG
â”‚   â”œâ”€â”€ monitoring/             # Monitoring & alerts
â”‚   â”‚   â””â”€â”€ alerting.py
â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚       â”œâ”€â”€ logger.py           # Logging configuration
â”‚       â””â”€â”€ rate_limiter.py     # API rate limiting
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ .env.example                # Example environment file
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ”§ Configuration

### Pipeline Configuration (`config/pipeline_config.yaml`)

```yaml
extraction:
  batch_size: 100
  retry_attempts: 3
  timeout: 30

transformation:
  remove_duplicates: true
  fill_missing: true
  outlier_threshold: 3

loading:
  batch_size: 1000
  upsert_enabled: true
```

### Data Source Configuration (`config/sources.yaml`)

```yaml
alpha_vantage:
  rate_limit: 5  # requests per minute
  symbols:
    - AAPL
    - GOOGL
    - MSFT

fred:
  series:
    - GDP
    - UNRATE
    - CPIAUCSL


## ğŸ“Š Database Schema

### Stock Prices Table

```sql
CREATE TABLE stock_prices (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(10,2),
    high DECIMAL(10,2),
    low DECIMAL(10,2),
    close DECIMAL(10,2),
    volume BIGINT,
    adjusted_close DECIMAL(10,2),
    data_source VARCHAR(50),
    extraction_timestamp TIMESTAMP,
    UNIQUE(symbol, date)
);
```

### Crypto Prices Table

```sql
CREATE TABLE crypto_prices (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    price_usd DECIMAL(15,2),
    volume_24h DECIMAL(20,2),
    market_cap DECIMAL(20,2),
    change_24h DECIMAL(5,2),
    data_source VARCHAR(50),
    extraction_timestamp TIMESTAMP,
    UNIQUE(symbol, timestamp)
);
```

### Economic Indicators Table

```sql
CREATE TABLE economic_indicators (
    id SERIAL PRIMARY KEY,
    series_id VARCHAR(20) NOT NULL,
    date DATE NOT NULL,
    value DECIMAL(15,4),
    series_name VARCHAR(200),
    units VARCHAR(50),
    frequency VARCHAR(20),
    data_source VARCHAR(50),
    extraction_timestamp TIMESTAMP,
    UNIQUE(series_id, date)
);
```

## ğŸ“ˆ Monitoring

### Metrics Tracked

- Pipeline execution time
- Records extracted/transformed/loaded
- Error rates by component
- API response times
- Database load times

### Alerting

Configure alerts in `src/monitoring/alerting.py`:
- Pipeline failures
- Data quality issues
- API rate limit warnings
- Unusual data patterns

## ğŸ” Security Best Practices

1. **Never commit API keys** - Use `.env` file
2. **Rotate credentials** regularly
3. **Use read-only database users** where possible
4. **Enable SSL/TLS** for database connections
5. **Implement rate limiting** to avoid API bans
6. **Monitor access logs** for unusual activity

## ğŸ› Troubleshooting

### Common Issues

**Issue**: Airflow DAG not appearing
```bash
# Check DAG syntax
python src/orchestration/dags/etl_pipeline.py

# Check Airflow logs
docker-compose logs airflow-scheduler
```

**Issue**: API rate limit exceeded
```bash
# Adjust rate limits in config/sources.yaml
# Or increase delay between requests
```

**Issue**: Database connection failed
```bash
# Verify Supabase credentials
# Check network connectivity
# Ensure database exists
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

## ğŸ™ Acknowledgments

- Alpha Vantage for stock market data
- FRED for economic indicators
- Finnhub for real-time market data
- Apache Airflow for orchestration
- Supabase for database hosting